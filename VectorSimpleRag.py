from groq import Groq
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# üîë Groq API key
client = Groq(api_key = os.getenv("GROQ_API_KEY"))

# üîπ 1. –ú—ç–¥—ç—ç–ª—ç–ª –∞—á–∞–∞–ª–∞—Ö
with open("data.txt", "r", encoding="utf-8", errors="ignore") as f:
    texts = f.read().split("\n")

# üîπ 2. Embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(texts)

# üîπ 3. FAISS –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç—Ö
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))

# üîπ 4. –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç
question = input("üßë‚Äçüíª –ê—Å—É—É–ª—Ç: ")
q_emb = model.encode([question])
_, I = index.search(q_emb, k=3)  # top-3 —Ö–æ–ª–±–æ–æ—Ç–æ–π ”©–≥“Ø“Ø–ª–±—ç—Ä

# üîπ 5. –•–æ–ª–±–æ–≥–¥–æ—Ö ”©–≥“Ø“Ø–ª–±—ç—Ä“Ø“Ø–¥–∏–π–≥ —Ç–∞—Ç–∞—Ö
context = "\n".join([texts[i] for i in I[0]])

# üîπ 6. LLM —Ä“Ø“Ø –∏–ª–≥—ç—ç—Ö
response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[
        {"role": "system", "content": "–¢–∞ ”©–≥”©–≥–¥—Å”©–Ω –º—ç–¥–ª—ç–≥ –¥—ç—ç—Ä “Ø–Ω–¥—ç—Å–ª—ç–Ω –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–¥–∞–≥ —Ç—É—Å–ª–∞—Ö —é–º."},
        {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–ê—Å—É—É–ª—Ç: {question}"}
    ],
    temperature=0.7,
    max_tokens=800,
)

print("\nü§ñ:", response.choices[0].message.content)
